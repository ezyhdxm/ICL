{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51ece98b-8697-4379-b4ea-141d79b75eb4",
   "metadata": {},
   "source": [
    "By vectorization, our implemetation of the [bigram data model](https://arxiv.org/pdf/2306.00802) is much faster compared to the original implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b93487c-316d-4694-9b63-a1017ab05f7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (markov.py, line 330)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m~/anaconda3/envs/ICL/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3577\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 23\u001b[0;36m\n\u001b[0;31m    from markov import *\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m~/Downloads/LLM/ICL/markov.py:330\u001b[0;36m\u001b[0m\n\u001b[0;31m    self.\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "import itertools\n",
    "import logging\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import sys\n",
    "\n",
    "from collections import namedtuple\n",
    "from functools import partial\n",
    "\n",
    "import jax\n",
    "from jax import jit, lax\n",
    "from jax import numpy as jnp\n",
    "from jax import random as jr\n",
    "from jax import vmap\n",
    "from jax.numpy import linalg as jla\n",
    "\n",
    "from typing import List, Optional, Tuple, Sequence\n",
    "\n",
    "from markov import *\n",
    "from config import *\n",
    "from causal_graph import *\n",
    "from old_sampler import *\n",
    "import torch\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02b00e64-febc-42bc-be3c-bd1c311dab7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.84 ms ± 10.6 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "args = DataArgs(k=2, seq_length=16, show_latents=False)\n",
    "ds = Dataset(args)\n",
    "rng = np.random.default_rng(42)\n",
    "%timeit ds.gen_batch(rng, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "658175a6-40e4-48a6-94f8-33a35b8f1637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "683 μs ± 9.11 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "torch_args = BiettiSamplerConfig(batch_size=16, seq_len=16, show_latents=True, seed=42)\n",
    "torch_ds = BiettiTask(torch_args)\n",
    "%timeit torch_ds.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a31b1af3-a770-4dc5-88ba-2fa2a9e7ebe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "428 μs ± 1.16 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "torch_args = BiettiSamplerConfig(batch_size=16, seq_len=16, show_mask=True)\n",
    "torch_ds = BBTask(torch_args)\n",
    "%timeit torch_ds.generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e9d36b-267f-4cfc-8e36-baeb9d5357cb",
   "metadata": {},
   "source": [
    "### Compared with JAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6151891-6c04-4348-b738-48985138b589",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stationary(pi):\n",
    "    mu = jla.svd(pi.T - jnp.eye(pi.shape[0]))[-1][-1]\n",
    "    return mu / mu.sum()\n",
    "\n",
    "\n",
    "class InContextTree:\n",
    "    def __init__(self, vocab_size, dag, alpha=1):\n",
    "        assert jnp.all(dag < jnp.arange(len(dag)))\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dag = dag\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def sample(self, key):\n",
    "        pi_key, seq_key, test_key = jr.split(key, 3)\n",
    "        prior = self.alpha * jnp.ones(self.vocab_size)\n",
    "        pi = jr.dirichlet(pi_key, prior, [self.vocab_size])\n",
    "        mu = get_stationary(pi)\n",
    "        x = jnp.zeros((len(self.dag) + 1,), dtype=int)\n",
    "\n",
    "        def step(i, carry):\n",
    "            x, k = carry\n",
    "            k, subkey = jr.split(k)\n",
    "            p = jnp.where(self.dag[i] == -1, mu, pi[x[self.dag[i]]])\n",
    "            x = x.at[i].set(jr.choice(subkey, pi.shape[0], p=p))\n",
    "            return x, k\n",
    "\n",
    "        x, _ = lax.fori_loop(0, len(self.dag), step, (x, seq_key))\n",
    "        test_token = jr.choice(test_key, self.vocab_size)\n",
    "        x = x.at[-1].set(test_token)\n",
    "        y = pi[test_token]\n",
    "        return x, y\n",
    "\n",
    "    def bayes(self, seq):\n",
    "        s, seq = seq[-1], seq[:-1]\n",
    "        counts = jnp.zeros(self.vocab_size)\n",
    "        counts = counts.at[seq].add(seq[self.dag] == s)\n",
    "        counts += self.alpha\n",
    "        return counts / counts.sum()\n",
    "\n",
    "\n",
    "class InContextDAG:\n",
    "    def __init__(self, vocab_size, dag, alpha):\n",
    "        for i, p in enumerate(dag):\n",
    "            # print(i, p)\n",
    "            assert max(p, default=-1) < i\n",
    "        dag = [jnp.array(p, dtype=int) for p in dag]\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dag = dag\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def sample(self, key):\n",
    "        pi_key, seq_key = jr.split(key)\n",
    "        ks = set(len(p) for p in self.dag)\n",
    "        pi_keys = jr.split(pi_key, len(ks))\n",
    "        pi = dict()\n",
    "        pi[0] = jnp.ones(self.vocab_size) / self.vocab_size\n",
    "        prior = self.alpha * jnp.ones(self.vocab_size)\n",
    "        for k, subkey in zip(ks, pi_keys):\n",
    "            pi[k] = jr.dirichlet(subkey, prior, [self.vocab_size] * k)\n",
    "\n",
    "        x = jnp.zeros((len(self.dag) - 1,), dtype=int)\n",
    "        for i in range(len(self.dag)):\n",
    "            k = len(self.dag[i])\n",
    "            if k == 0:\n",
    "                p = pi[0]\n",
    "            else:\n",
    "                p = pi[k][tuple(x[self.dag[i]])]\n",
    "\n",
    "            if i != len(self.dag) - 1:\n",
    "                seq_key, subkey = jr.split(seq_key)\n",
    "                new_token = jr.choice(subkey, self.vocab_size, p=p)\n",
    "                x = x.at[i].set(new_token)\n",
    "        return x, p\n",
    "\n",
    "    def bayes(self, seq):\n",
    "        counts = jnp.zeros(self.vocab_size)\n",
    "        s = seq[self.dag[-1]]\n",
    "        for i in range(len(self.dag) - 1):\n",
    "            if len(self.dag[i]) == len(s):\n",
    "                counts = counts.at[seq[i]].add(jnp.all(seq[self.dag[i]] == s))\n",
    "        counts += self.alpha\n",
    "        return counts / counts.sum()\n",
    "\n",
    "class RNG:\n",
    "    def __init__(self, seed=None, key=None):\n",
    "        if seed is not None:\n",
    "            self.key = jax.random.PRNGKey(seed)\n",
    "        elif key is not None:\n",
    "            self.key = key\n",
    "        else:\n",
    "            raise Exception(\"RNG expects either a seed or random key.\")\n",
    "\n",
    "    def next(self, n_keys=1):\n",
    "        if n_keys > 1:\n",
    "            return jax.random.split(self.next(), n_keys)\n",
    "        else:\n",
    "            self.key, key = jax.random.split(self.key)\n",
    "            return key\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        return partial(getattr(jax.random, name), self.next())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264a8c58-f3cf-4380-bcd6-16a75a42b67d",
   "metadata": {},
   "source": [
    "JAX is slightly faster than the vectorized PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b632dbb-da1e-41bd-86b9-f7fe12cb9c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:2025-01-28 15:03:01,126:jax._src.xla_bridge:927: Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "INFO:2025-01-28 15:03:01,128:jax._src.xla_bridge:927: Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: dlopen(libtpu.so, 0x0001): tried: 'libtpu.so' (no such file), '/System/Volumes/Preboot/Cryptexes/OSlibtpu.so' (no such file), '/Users/hyan/anaconda3/envs/ICL/bin/../lib/libtpu.so' (no such file), '/usr/lib/libtpu.so' (no such file, not in dyld cache), 'libtpu.so' (no such file), '/usr/local/lib/libtpu.so' (no such file), '/usr/lib/libtpu.so' (no such file, not in dyld cache)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10\n",
    "seq_len = 256\n",
    "batch_size = 64\n",
    "rng = RNG(0)\n",
    "dag = jnp.arange(seq_len)-1\n",
    "BigramJax = InContextTree(vocab_size, dag)\n",
    "sample_fn = jit(lambda k: vmap(BigramJax.sample)(jr.split(k, batch_size)))\n",
    "key, subkey = jr.split(rng.next())\n",
    "batches = sample_fn(subkey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd56dbf8-5c82-4869-94d5-d1e636cdbbe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.18 ms ± 43.3 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "key, subkey = jr.split(rng.next())\n",
    "%timeit sample_fn(subkey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3d6625f-2418-4980-8bb5-e4d3f5316567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.43 ms ± 147 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "torch_args = MarkovSamplerConfig(vocab_size=vocab_size, batch_size=batch_size, seq_len=seq_len, order=1)\n",
    "torch_ds = MarkovSampler(torch_args)\n",
    "%timeit torch_ds.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abf078d2-0033-4983-a136-5a6ca56e77e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.71 ms ± 266 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "dag = torch.arange(seq_len)-1\n",
    "BigramTorch = InContextTreeTorch(vocab_size, dag)\n",
    "%timeit BigramTorch.sample_batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2021aca-eaec-45ca-99e9-47a228a2bef3",
   "metadata": {},
   "source": [
    "### Torch with jit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bb9291-9277-4f38-9cf3-a96b780e9179",
   "metadata": {},
   "source": [
    "We can also use torch.jit to optimize the torch implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d7a8e14-b92e-4277-8c3d-4a313fc84ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import torch\n",
    "\n",
    "def get_stationary(pi: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Cannot be jitted due to dynamic tensor shape\n",
    "    \"\"\"\n",
    "    batch_size, vocab_size, _ = pi.shape\n",
    "    pi_t = pi.transpose(1, 2)  # Transpose each matrix\n",
    "    svd_input = pi_t - torch.eye(vocab_size, device=pi.device).unsqueeze(0).expand(batch_size, -1, -1)\n",
    "    _, _, v = torch.linalg.svd(svd_input)\n",
    "    mu = torch.abs(v[:, -1, :])  # Last singular vector for each matrix, make sure that mu is positive to eliminate numerical issues\n",
    "    return mu / mu.sum(dim=1, keepdim=True)\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def sample_dirichlet(alpha: torch.Tensor, size: List[int]) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Manually sample from a Dirichlet distribution using Gamma sampling due to incompatibility\n",
    "    between torch.jit and torch.distributions.Dirichlet. \n",
    "\n",
    "    Args:\n",
    "        alpha (torch.Tensor): Concentration parameters of shape (num_states,).\n",
    "        size (List[int]): Output size, e.g., [batch_size, num_states, num_states].\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Samples from the Dirichlet distribution of shape `size`.\n",
    "    \"\"\"\n",
    "    gamma_samples = torch._standard_gamma(alpha.expand(size))\n",
    "    return gamma_samples / gamma_samples.sum(dim=-1, keepdim=True)\n",
    "\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def markov_chain_sample_batch(num_states: int, batch_size: int, dag: torch.Tensor,\n",
    "                              device: str, alpha: float = 0.1) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Sample a batch of sequences from a Markov chain.\n",
    "\n",
    "    Args:\n",
    "        alpha (float): Dirichlet prior concentration parameter.\n",
    "        num_states (int): Number of states in the Markov chain.\n",
    "        device (str): Device to perform computations on ('cpu' or 'cuda').\n",
    "        seq_len (int): Length of the sequence to sample.\n",
    "        batch_size (int): Number of sequences to sample.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Batch of sampled sequences of shape (batch_size, seq_len).\n",
    "    \"\"\"\n",
    "    # Sample transition matrices from a Dirichlet distribution\n",
    "    prior = alpha * torch.ones(num_states, device=device)\n",
    "    pi = sample_dirichlet(prior, size=(batch_size, num_states, num_states))  # Shape: (batch_size, num_states, num_states)\n",
    "    \n",
    "    # Compute stationary distribution\n",
    "    mu = get_stationary(pi)\n",
    "    \n",
    "    seq_len = len(dag) + 1\n",
    "\n",
    "    # Initialize sequences\n",
    "    samples = torch.zeros((batch_size, seq_len), dtype=torch.long, device=device)\n",
    "\n",
    "    for t in range(seq_len-1):\n",
    "        if dag[t] == -1:  # Root node\n",
    "            p = mu  # Use stationary distribution\n",
    "        else:  # Child node\n",
    "            parent_tokens = samples[:, dag[t]]  # Shape: (batch_size,)\n",
    "            p = pi[torch.arange(batch_size), parent_tokens]  # Transition probabilities for parent tokens\n",
    "\n",
    "        # Sample tokens for all sequences in the batch\n",
    "        samples[:, t] = torch.multinomial(p, num_samples=1).squeeze()\n",
    "    \n",
    "    # Sample test tokens for the last position\n",
    "    test_tokens = torch.randint(num_states, (batch_size,), device=device)\n",
    "    samples[:, -1] = test_tokens\n",
    "    target_probs = pi[torch.arange(batch_size), test_tokens]  # Probabilities of test tokens\n",
    "    return samples, target_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f6ebe6d-41d7-4f71-b2af-b50303bbe890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a batch of sequences\n",
    "samples_batch = markov_chain_sample_batch(vocab_size, seq_len, dag, \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5269383e-47e2-4366-afd9-b008ce106824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.7 ms ± 41.5 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit markov_chain_sample_batch(vocab_size, seq_len, dag, \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8372dd-7387-4162-90f2-2e5f00066ae8",
   "metadata": {},
   "source": [
    "It is even slower, so we do not bother..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6e223e-254c-44aa-8368-2f786d1d3e4f",
   "metadata": {},
   "source": [
    "### Test Multiple Parents Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46c732b5-5278-40a1-99ef-2a17a563295c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10\n",
    "seq_len = 256\n",
    "alpha = 1\n",
    "batch_size = 64\n",
    "dag = [[], []] + [[(i - 1) // 2, i - 1] for i in range(2, seq_len + 1)]\n",
    "\n",
    "problem = InContextDAG(vocab_size=vocab_size, dag=dag, alpha=alpha)\n",
    "sample_fn = jit(lambda k: vmap(problem.sample)(jr.split(k, batch_size)))\n",
    "rng = RNG(0)\n",
    "batch, p = sample_fn(rng.next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7855083-dac9-43e1-9b42-75d9836b653c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.3 ms ± 109 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sample_fn(rng.next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66f69b69-3b96-4062-b08b-7925d0893fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "problemTorch = InContextDAGTorch(vocab_size=vocab_size, dag=dag, alpha=alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba918c5-0e70-48e7-94e2-48fb78dc1624",
   "metadata": {},
   "source": [
    "For multiple parents, our implementation is much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfccfe37-5aff-454e-baca-1b9a0af4cfda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 ms ± 27.6 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit problemTorch.sample_batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc8ff0ae-2d33-4458-91ca-cb12fb1ab983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.24 μs ± 73.3 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "batch_size = 2\n",
    "seq_len = 6\n",
    "order = 2\n",
    "batch = torch.randn((batch_size, seq_len))\n",
    "states = torch.stack([batch[:, t:t + order] for t in range(seq_len - order)], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "032d1927-a6a3-459f-a763-7f1d93089267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.85 μs ± 37.8 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "batch_size = 2\n",
    "seq_len = 6\n",
    "order = 2\n",
    "batch = torch.randn((batch_size, seq_len))\n",
    "\n",
    "states = torch.as_strided(batch, \n",
    "                 size=(batch_size, seq_len - order, order), \n",
    "                 stride=(batch.stride(0), batch.stride(1), batch.stride(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684b5620-54a3-405e-a612-99a1e0008b4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ICL]",
   "language": "python",
   "name": "conda-env-ICL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
