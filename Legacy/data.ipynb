{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8da27af1-94b8-4479-9f98-dc816c00794f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters: 1,115,394\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import pickle\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "__file__ = \"\"\n",
    "input_file_path = os.path.join(os.path.dirname(__file__), 'input.txt')\n",
    "if not os.path.exists(input_file_path):\n",
    "    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "    with open(input_file_path, 'w') as f:\n",
    "        f.write(requests.get(data_url).text)\n",
    "with open(input_file_path, 'r') as f:\n",
    "    data = f.read()\n",
    "print(f\"length of dataset in characters: {len(data):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "961abb22-d62d-4a70-9ee5-d9886b4b406e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all the unique characters: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "vocab size: 65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(data)))\n",
    "vocab_size = len(chars)\n",
    "print(\"all the unique characters:\", ''.join(chars))\n",
    "print(f\"vocab size: {vocab_size:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbed4b9c-0ed3-4b12-868c-4cabcb5ffe39",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for i,ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6012333-9ec6-47d4-b7e6-b1fb7359dd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7d96995d-8a00-4869-951f-5a6241dd0080",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams = dict(Counter(data))\n",
    "bigrams = dict(Counter(chain(zip(data[::2], data[1::2]), zip(data[1::2], data[2::2]))))\n",
    "bigrams_cond = defaultdict(dict)\n",
    "for (w1, w2), cnt in bigrams.items():\n",
    "    bigrams_cond[w1][w2] = cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3e2ce77e-59f5-4302-9710-6deee27fef84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import itertools \n",
    "import logging\n",
    "import random \n",
    "import math\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import sys\n",
    "from typing import List, Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "edf04e5e-0957-4a54-91c6-13ed51056a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bdbbcb29-a51e-4086-91a5-ace98206597b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataArgs:\n",
    "    k: int = 0\n",
    "    seq_length: int = 256\n",
    "    show_latents: bool = False\n",
    "    fixed_special_toks: bool = False\n",
    "    special_toks_offset: int = 0\n",
    "    output_counter: bool = True\n",
    "    no_repeat: bool = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f13f6e3a-64ae-48a4-8c3d-d0b1f284a933",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, args: DataArgs,\n",
    "                 train_test: Optional[str] = None,\n",
    "                 bigram_outs: Optional[bool] = False):\n",
    "        self.k = args.k\n",
    "        self.seq_length = args.seq_length\n",
    "        self.show_latents = args.show_latents\n",
    "        self.train_test = train_test\n",
    "        self.output_counter = args.output_counter\n",
    "        self.no_repeat = args.no_repeat\n",
    "        self.bigram_outs = bigram_outs\n",
    "\n",
    "        # init distributions\n",
    "        self.meta = pickle.load(open('data/meta.pkl', 'rb'))\n",
    "        self.itos = self.meta['itos']\n",
    "        self.stoi = self.meta['stoi']\n",
    "        self.num_tokens = self.meta['vocab_size']\n",
    "        self.tok_range = list(np.arange(self.num_tokens))\n",
    "        self.n_train_toks = self.num_tokens\n",
    "\n",
    "        self.marginal = np.zeros(self.num_tokens)\n",
    "        for k, cnt in self.meta['unigrams'].items():\n",
    "            self.marginal[self.stoi[k]] = cnt\n",
    "        self.marginal /= self.marginal.sum()\n",
    "\n",
    "         # conditionals\n",
    "        self.cond = [np.zeros(self.num_tokens) for _ in range(self.num_tokens)]\n",
    "        for (w1, w2), cnt in self.meta['bigrams'].items():\n",
    "            self.cond[self.stoi[w1]][self.stoi[w2]] += cnt\n",
    "        for i in range(self.num_tokens):\n",
    "            self.cond[i] /= self.cond[i].sum()\n",
    "\n",
    "        # special tokens\n",
    "        self.idxs = None\n",
    "        if args.fixed_special_toks:\n",
    "            # use unigram marginals\n",
    "            self.idxs = list(self.marginal.argsort()[\n",
    "                             self.num_tokens-self.k-args.special_toks_offset:self.num_tokens-args.special_toks_offset])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2199cc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "vocab_size = 3\n",
    "alpha = 1.\n",
    "order = 1\n",
    "rho = 0.5\n",
    "num_state_order = vocab_size * order\n",
    "random_rows_size = int(rho * num_state_order)\n",
    "batch_size = 4\n",
    "epochs = 1\n",
    "num_samples = epochs * batch_size\n",
    "\n",
    "\n",
    "dirichlet_dist = torch.distributions.Dirichlet(torch.ones(vocab_size, device=\"cpu\")*alpha)\n",
    "base_trans_mat = dirichlet_dist.sample((num_state_order,))\n",
    "base_trans_mat /= base_trans_mat.sum(dim=-1, keepdim=True)\n",
    "random_rows = torch.randperm(num_state_order)[:random_row_size]\n",
    "\n",
    "\n",
    "\n",
    "trans_mat = base_trans_mat.unsqueeze(0).repeat((num_samples, 1, 1)) # Shape: (num_samples, num_states_order, num_states)\n",
    "trans_random = dirichlet_dist.sample((num_samples, random_rows_size,))  # Shape: (num_samples, random_rows_size, num_states)\n",
    "trans_mat[:, random_rows] = trans_random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bc9c2912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed Random Markov chain sampler\n",
    "class FRMarkovSampler:\n",
    "    def __init__(self, config):\n",
    "        self.seq_len = config.seq_len\n",
    "        self.num_states = config.vocab_size\n",
    "        self.order = config.order\n",
    "        self.num_states_order = self.num_states ** self.order\n",
    "        self.batch_size = config.batch_size\n",
    "        self.test_size = config.test_size\n",
    "        self.device = config.device\n",
    "        self.dirichlet_dist = torch.distributions.Dirichlet(torch.ones(self.num_states, device=self.device)*config.alpha)\n",
    "        self.powers = (self.num_states ** torch.arange(self.order - 1, -1, -1, device=self.device)).long()\n",
    "        # Sample all transition probabilities in one go\n",
    "        self.base_trans_matrix = self.dirichlet_dist.sample((self.num_states_order,))  # Shape: (num_states_order, num_states)\n",
    "        self.base_trans_matrix /= self.base_trans_matrix.sum(dim=1, keepdim=True)\n",
    "        self.random_rows_size = int(config.rho * self.num_states_order) # proportion of rows that have a random transition\n",
    "        self.random_rows = torch.randperm(self.num_states_order)[:self.random_rows_size] # pick random rows\n",
    "        \n",
    "    \n",
    "    def generate(self, epochs=1, mode:str=\"train\")-> torch.Tensor:\n",
    "        num_samples = self.batch_size if mode == \"train\" else self.test_size\n",
    "        num_samples *= epochs\n",
    "        trans_mat = self.base_trans_matrix.unsqueeze(0).repeat((num_samples, 1, 1)) # Shape: (num_samples, num_states_order, num_states)\n",
    "        trans_random = self.dirichlet_dist.sample((num_samples, self.random_rows_size,))  # Shape: (num_samples, random_rows_size, num_states)\n",
    "        trans_mat[:, self.random_rows] = trans_random\n",
    "        \n",
    "        range_vecs = torch.arange(num_samples, device=self.device)\n",
    "        \n",
    "        # Initialize the samples tensor\n",
    "        samples = torch.zeros((num_samples, self.seq_len), dtype=torch.long, device=self.device)\n",
    "        \n",
    "        state = torch.randint(high=self.num_states, size=(num_samples, self.order), device=self.device) # Shape: (num_samples, order)\n",
    "        samples[:, :self.order] = state\n",
    "            \n",
    "        for t in range(self.order, self.seq_len):\n",
    "            state_indices = torch.sum(state * self.powers, dim=1) #shape: (num_samples,)\n",
    "            probs = trans_mat[range_vecs, state_indices, :]  # Shape: (num_samples, num_states)\n",
    "            \n",
    "            # Sample the next states for the entire batch\n",
    "            next_states = torch.multinomial(probs, num_samples=1).squeeze(1)\n",
    "            \n",
    "            # Update the sequence with the sampled next states\n",
    "            samples[:, t] = next_states\n",
    "            \n",
    "            # Update the state window (shift left and append the new state)\n",
    "            state[:, :-1] = state[:, 1:]  # Shift left\n",
    "            state[:, -1] = next_states    # Append new state\n",
    "        \n",
    "        return samples.reshape(epochs, -1, self.seq_len), probs.reshape(epochs, -1, self.num_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8907c82e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[2, 1, 2, 0, 0, 1, 1, 1, 1, 2, 0, 0, 1, 1, 1, 2, 1, 2, 1, 2],\n",
       "          [0, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 2, 1, 2, 1, 2],\n",
       "          [1, 1, 1, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0],\n",
       "          [1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 0, 0, 1, 2, 0, 2, 0, 2]]]),\n",
       " tensor([[[0.0060, 0.0371, 0.9569],\n",
       "          [0.0060, 0.0371, 0.9569],\n",
       "          [0.0801, 0.1762, 0.7437],\n",
       "          [0.5488, 0.2621, 0.1891]]]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class test_config:\n",
    "    seq_len:int = 20\n",
    "    vocab_size:int = 3\n",
    "    order:int = 2\n",
    "    batch_size:int = 4\n",
    "    test_size:int = 3\n",
    "    device:str = \"cpu\"\n",
    "    alpha:float = 1.\n",
    "    rho:float = 0.2\n",
    "\n",
    "config = test_config\n",
    "sampler = FRMarkovSampler(config)\n",
    "sampler.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d3f816",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
