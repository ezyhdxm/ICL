# Instructions

To run the current ongoing experiments, see this [notebook](./TriggerMarkov.ipynb) for random triggers and this [notebook](./LatentModel.ipynb) for latent markov models.  

The structure of this repositary is roughly summarized in the following file tree:

ICL/
â”œâ”€â”€ checkpoints/          # checkpoints for models
â”œâ”€â”€ tasks/
â”‚   â”œâ”€â”€ markov.py         # classes for different task samplers. 
â”‚   â”œâ”€â”€ causal_graph.py   # legacy class for causal graph samplers.
â”‚   â”œâ”€â”€ test_markov.py    # classes for the latent markov task sampler. 
â”‚   â””â”€â”€ old_sampler.py    # legacy samplers.
â”‚ 
â”‚   
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ ngram_learner.py  # empircal n-gram learners, used to provide baselines.
â”‚   â”œâ”€â”€ attention.py      # implementation of multi-head attention. 
â”‚   â”œâ”€â”€ base_models.py    # implementation of the base transformer model.
â”‚   â”œâ”€â”€ pos_encoder.py    # implementation of different positional encoding classes.
â”‚   â””â”€â”€ sae.py            # sparse autoencoder, not in use currently.
â”œâ”€â”€ figures/
â”‚   â”œâ”€â”€ head_view.py      # Bertviz-type attention visualization.
â”‚   â”œâ”€â”€ view_util.py      # Bertviz-type attention visualization utility file.
â”‚   â”œâ”€â”€ head_view.js      # Bertviz-type attention visualization javascript.
â”‚   â”œâ”€â”€ test_head_view.js # failed attempt to transpose Bertviz-type attention visualization.
â”‚   â””â”€â”€ plot.py           # create plots for training loss/ memory probes/ average attention pattern/ etc.
â”‚ 
â”œâ”€â”€ TriggerMarkov.ipynb   # Entry point to run random trigger experiments.
â”œâ”€â”€ LatentModel.ipynb     # Entry point to run latent markov experiments.
â”œâ”€â”€ train.py              # trains the model, generates training statistics and creates plots.
â”œâ”€â”€ config.py             # configuration classes for sampler and tasks.
â”œâ”€â”€ train_utils.py        # some utilities to get training loss and memory probe statistics. 
â”œâ”€â”€ util.py               # implementation of different probes.
â””â”€â”€ README.md



## Legacy

We are interested in understanding how transformers develop the in-context-learning ability through gradient descent. This repo reproduces implemetations of a few ICL experiments. Our implemetation is more efficient compared to some of the original implementations (see the [notebook](./Legacy/SpeedTest.ipynb) for more details). For demostrations of experiments in the previous literature, see the [notebook](./Legacy/LiterReview.ipynb). 

### ðŸ“– Citations

```bibtex
@article{edelman2024evolution,
  title={The evolution of statistical induction heads: In-context learning markov chains},
  author={Edelman, Benjamin L and Edelman, Ezra and Goel, Surbhi and Malach, Eran and Tsilivis, Nikolaos},
  journal={arXiv preprint arXiv:2402.11004},
  year={2024}
}
```
```bibtex
@article{bietti2024birth,
  title={Birth of a transformer: A memory viewpoint},
  author={Bietti, Alberto and Cabannes, Vivien and Bouchacourt, Diane and Jegou, Herve and Bottou, Leon},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
```
```bibtex
@article{guo2024active,
  title={Active-dormant attention heads: Mechanistically demystifying extreme-token phenomena in llms},
  author={Guo, Tianyu and Pai, Druv and Bai, Yu and Jiao, Jiantao and Jordan, Michael I and Mei, Song},
  journal={arXiv preprint arXiv:2410.13835},
  year={2024}
}
```

```bibtex
@article{nichani2024transformers,
  title={How transformers learn causal structure with gradient descent},
  author={Nichani, Eshaan and Damian, Alex and Lee, Jason D},
  journal={arXiv preprint arXiv:2402.14735},
  year={2024}
}
```

```bibtex
@article{rajaraman2024transformers,
  title={Transformers on markov data: Constant depth suffices},
  author={Rajaraman, Nived and Bondaschi, Marco and Ramchandran, Kannan and Gastpar, Michael and Makkuva, Ashok Vardhan},
  journal={arXiv preprint arXiv:2407.17686},
  year={2024}
}
```

