{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51ece98b-8697-4379-b4ea-141d79b75eb4",
   "metadata": {},
   "source": [
    "By vectorization, our implemetation of the [bigram data model](https://arxiv.org/pdf/2306.00802) is much faster compared to the original implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b93487c-316d-4694-9b63-a1017ab05f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import itertools\n",
    "import logging\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import sys\n",
    "\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "from markov import BiettiTask\n",
    "import torch\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataArgs:\n",
    "    k: int = 0\n",
    "    seq_length: int = 256\n",
    "    show_latents: bool = False\n",
    "    fixed_special_toks: bool = False\n",
    "    special_toks_offset: int = 0\n",
    "    output_counter: bool = False\n",
    "    no_repeat: bool = False\n",
    "    vocab_size: int = 20\n",
    "\n",
    "@dataclass\n",
    "class BiettiConfig:\n",
    "    seq_len: int = 256\n",
    "    vocab_size: int = 20\n",
    "    marginal: torch.Tensor = torch.ones((vocab_size,)) / vocab_size\n",
    "    trans_mat: torch.Tensor = torch.ones((vocab_size, vocab_size)) / vocab_size\n",
    "    batch_size: int = 1024\n",
    "    test_size: int = 4096\n",
    "    k: int = 2\n",
    "    show_latents: bool = False\n",
    "    seed: int = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "239101a8-1afe-4187-8306-1551104b3f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Modified Original Implemetation #\n",
    "###################################\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, args: DataArgs,\n",
    "                 train_test: Optional[str] = None,\n",
    "                 bigram_outs: Optional[bool] = True):\n",
    "        self.k = args.k\n",
    "        self.seq_length = args.seq_length\n",
    "        self.show_latents = args.show_latents\n",
    "        self.train_test = train_test\n",
    "        self.output_counter = args.output_counter\n",
    "        self.no_repeat = args.no_repeat\n",
    "        self.vocab_size = args.vocab_size\n",
    "        self.bigram_outs = bigram_outs\n",
    "\n",
    "        self.num_tokens = self.vocab_size\n",
    "        self.tok_range = list(np.arange(self.num_tokens))\n",
    "\n",
    "        self.n_train_toks = self.num_tokens\n",
    "        self.marginal = np.ones(self.num_tokens) / self.num_tokens\n",
    "        self.cond = [np.ones(self.num_tokens) / self.num_tokens for _ in range(self.num_tokens)]\n",
    "\n",
    "        # special tokens\n",
    "        self.idxs = None\n",
    "\n",
    "\n",
    "    def gen_seq(self, rng: np.random.Generator):\n",
    "        # select special tokens for this sequence\n",
    "        idxs = list(rng.choice(self.tok_range, p=self.marginal, size=self.k, replace=False))\n",
    "        \n",
    "        if self.no_repeat:  # prevent next token to be same as idx\n",
    "            pools = [self.tok_range.copy() for idx in idxs]\n",
    "            for i, idx in enumerate(idxs):\n",
    "                pools[i].remove(idx)\n",
    "        else:\n",
    "            pools = [self.tok_range for idx in idxs]\n",
    "            \n",
    "        if self.bigram_outs:\n",
    "            outs = [rng.choice(pool, p=(self.cond[idx][pool] / self.cond[idx][pool].sum())) for pool, idx in zip(pools, idxs)]\n",
    "        else:\n",
    "            outs = [rng.choice(pool) for pool in pools]\n",
    "\n",
    "        # print([(idxs[i], outs[i]) for i in range(self.k)])\n",
    "        \n",
    "        cnts = {}\n",
    "\n",
    "        if self.show_latents:\n",
    "            seq = idxs.copy()\n",
    "            outputs_seq = [-1] * len(idxs) #  []\n",
    "        else:\n",
    "            seq = []\n",
    "            outputs_seq = []\n",
    "        seq += [rng.choice(self.tok_range, p=self.marginal)]\n",
    "        while len(seq) < self.seq_length + 1:\n",
    "            last = seq[-1]\n",
    "            if last in idxs:\n",
    "                seq.append(outs[idxs.index(last)])\n",
    "                if self.output_counter:\n",
    "                    cnts[last] = cnts.get(last, 0) + 1\n",
    "                    outputs_seq.append(cnts[last])\n",
    "                else:\n",
    "                    outputs_seq.append(1)\n",
    "            else:\n",
    "                probs = self.cond[last]\n",
    "                outputs_seq.append(0)\n",
    "                seq.append(rng.choice(self.tok_range, p=probs))\n",
    "        outputs_seq.append(0)\n",
    "\n",
    "        return seq, outputs_seq\n",
    "\n",
    "    def gen_seqs(self, rng: np.random.Generator) -> List[str]:\n",
    "        while True:\n",
    "            seq, outputs_seq = self.gen_seq(rng)\n",
    "            yield (seq, outputs_seq)\n",
    "\n",
    "    def gen_batch(self, rng: np.random.Generator, batch_size: int):\n",
    "        seqs = []\n",
    "        outs = []\n",
    "        for _ in range(batch_size):\n",
    "            seq, out = self.gen_seq(rng)\n",
    "            seqs += seq\n",
    "            outs += out\n",
    "        x = np.array(seqs).reshape(batch_size, self.seq_length + 1)\n",
    "        outs = np.array(outs).reshape(batch_size, self.seq_length + 1)\n",
    "        return x, outs\n",
    "\n",
    "\n",
    "def iterate_batches(dataset: Dataset,\n",
    "                    batch_size: int = 20,\n",
    "                    num_workers: int = 6,\n",
    "                    seed: int = 42):\n",
    "    def worker(queue, rng):\n",
    "        while True:\n",
    "            x, outs = dataset.gen_batch(rng, batch_size)\n",
    "            queue.put((x, outs))\n",
    "\n",
    "    import multiprocessing as mp\n",
    "    q = mp.Queue(maxsize=1000)\n",
    "    processes = [mp.Process(target=worker, args=(q, np.random.default_rng([seed, i]))) for i in range(num_workers)]\n",
    "    for p in processes:\n",
    "        p.start()\n",
    "\n",
    "    seq = []\n",
    "    outputs_seq = []\n",
    "    count = 0\n",
    "    try:\n",
    "        while True:\n",
    "            x, outs = q.get()\n",
    "            yield (x[:,:-1], x[:,1:], outs[:,:-1])\n",
    "    except:\n",
    "        for p in processes:\n",
    "            p.kill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02b00e64-febc-42bc-be3c-bd1c311dab7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1, 19, 15, 17, 15, 17,  2,  9,  7, 18, 12, 16,  8, 13,  4, 11,\n",
       "          1],\n",
       "        [19, 17, 15,  3,  9,  0,  3, 13, 14, 19,  6,  7,  9,  3,  2,  9,\n",
       "          4],\n",
       "        [ 6, 16, 16,  7,  5, 13, 16,  2,  3,  0, 15, 13, 16, 14, 15,  9,\n",
       "         11],\n",
       "        [15, 12, 11, 11,  6,  0,  8,  4,  8, 17,  4,  1,  5,  5, 13, 11,\n",
       "         11]]),\n",
       " array([[0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = DataArgs(k=2, seq_length=16, show_latents=False)\n",
    "ds = Dataset(args)\n",
    "rng = np.random.default_rng(42)\n",
    "ds.gen_batch(rng, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "658175a6-40e4-48a6-94f8-33a35b8f1637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 3,  0, 19, 10, 14, 13,  2,  8,  9, 15, 14,  5, 14, 19, 13, 14, 15,  7],\n",
       "         [ 4, 17,  5, 12,  7,  0,  4,  9, 16,  9, 13,  2, 12, 15,  1, 14,  8,  7],\n",
       "         [12,  7, 17, 18,  9, 17,  9, 10,  5, 11,  9, 15,  6, 17, 12, 17,  4, 14],\n",
       "         [15,  0, 10,  0, 16,  1,  7,  5, 19,  2,  7, 18,  6, 17, 10, 16,  6, 16]]),\n",
       " tensor([[-1, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [-1, -1,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [-1, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0],\n",
       "         [-1, -1,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_args = BiettiConfig(batch_size=4, seq_len=16, show_latents=True, seed=42)\n",
    "torch_ds = BiettiTask(torch_args)\n",
    "torch_ds.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31b1af3-a770-4dc5-88ba-2fa2a9e7ebe4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
