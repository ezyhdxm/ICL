{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4576b7f0-b1fa-463e-838f-928f3a42f2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from itertools import product\n",
    "from dataclasses import dataclass\n",
    "from collections import namedtuple, defaultdict\n",
    "from tasks.markov import *\n",
    "from models.base_models import *\n",
    "from models.pos_encoder import *\n",
    "from models.forward_tracer import *\n",
    "from tasks.test_markov import *\n",
    "from config import *\n",
    "from train import *\n",
    "import figures.plot as plot\n",
    "from figures.ood_plots import *\n",
    "from util import *\n",
    "import seaborn as sns\n",
    "import torch.utils.benchmark as benchmark\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import os\n",
    "import re\n",
    "\n",
    "from figures.head_view import *\n",
    "\n",
    "torch.set_printoptions(precision=3, sci_mode=False)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c63a70ed-c464-4a8d-a543-ce61d9391c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN, VOC_SIZE, BATCH_SIZE, alpha = 128, 5, 32, 1\n",
    "pos_enc=\"rotary\"\n",
    "hidden_dim=128\n",
    "mlp_layers=None\n",
    "epochs=10000\n",
    "num_layers=2\n",
    "\n",
    "flash = False\n",
    "    \n",
    "if pos_enc in [\"rotary\", \"abs\"]:\n",
    "    flash = True\n",
    "\n",
    "if mlp_layers is None:\n",
    "    mlp_layers = [i for i in range(num_layers)]\n",
    "    \n",
    "mlp = [False] * num_layers\n",
    "for i in mlp_layers:\n",
    "    mlp[i] = True \n",
    "mlp = tuple(mlp)\n",
    "\n",
    "config = Config(\n",
    "        emb_dim=hidden_dim,\n",
    "        num_layers=num_layers,\n",
    "        num_heads=tuple([1]*num_layers),\n",
    "        identity_query=False,\n",
    "        seq_len=SEQ_LEN,\n",
    "        vocab_size=VOC_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_epochs=epochs,\n",
    "        eval_iter=1000,\n",
    "        pos_enc=pos_enc,\n",
    "        pos_max_len=SEQ_LEN,\n",
    "        get_attn=500,\n",
    "        get_checkpoints=200,\n",
    "        mlp=mlp,\n",
    "        activation=mlp,\n",
    "        flash=flash, # Use rotary\n",
    "        ff_dim=hidden_dim,\n",
    "        layer_norm=False,\n",
    "        ngram=2,\n",
    "        learning_rate=1e-4,\n",
    "        task_name=\"icl-mc\"\n",
    "    )\n",
    "\n",
    "sampler_config = MarkovSamplerConfig(seq_len=SEQ_LEN, vocab_size=VOC_SIZE, order=1, alpha=alpha,\n",
    "                                     batch_size=BATCH_SIZE, task_name=\"icl-mc\")\n",
    "\n",
    "model = Transformer(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0ff2ff0-97a2-4529-af3b-4505fe9b7bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-1): 2 x TFBlock(\n",
       "    (MHA): MultiHeadAttention(\n",
       "      (query): Linear(in_features=128, out_features=128, bias=False)\n",
       "      (key): Linear(in_features=128, out_features=128, bias=False)\n",
       "      (value): Linear(in_features=128, out_features=128, bias=False)\n",
       "      (out): Linear(in_features=128, out_features=128, bias=False)\n",
       "    )\n",
       "    (ln1): Identity()\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (ln2): Identity()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_layers(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5a5b66-6a5d-4438-9f18-cd0da7cd9b09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
